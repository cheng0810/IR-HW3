{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from numba import jit\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "doc_dir = 'Document/'\n",
    "query_dir = 'Query/'\n",
    "\n",
    "# Hyperparameters\n",
    "min_df = 1         # Exclude words with document frequency < min_df for unigram and PLSA\n",
    "topic_num = 30     # Topic numbers of PLSA\n",
    "max_iter = 1000     # Max iterations of PLSA\n",
    "threshold = 1e-4   # Decide convergence of PLSA for early stopping (Refer to PLSA() below for details)\n",
    "\n",
    "alpha = 0.1   # Weight of unigram model\n",
    "beta = 0.7    # Weight of PLSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 168 ms, sys: 60 ms, total: 228 ms\n",
      "Wall time: 206 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Read 'Collections.txt' as train set\n",
    "train_set = []\n",
    "with open('Collection.txt', mode='r') as file:\n",
    "    train_set += [line.rstrip() for line in file]\n",
    "\n",
    "# Read documents as test set\n",
    "test_set = []\n",
    "doc_list = os.listdir(doc_dir)\n",
    "for doc_name in doc_list:\n",
    "    doc_path = doc_dir + doc_name\n",
    "    with open(doc_path, mode=\"r\") as file:\n",
    "        lines = file.readlines()[3:]   # First three lines are useless headers\n",
    "        lines = [line.rstrip()[:-3] for line in lines]   # Strip tailing -1 from each line\n",
    "        document = ' '.join(lines)\n",
    "        test_set.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.49 s, sys: 64 ms, total: 3.55 s\n",
      "Wall time: 3.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Process train set into doc-term matrix\n",
    "vectorizer = CountVectorizer(token_pattern='[0-9]+', min_df=min_df)\n",
    "doc_term = vectorizer.fit_transform(train_set).tocoo()  # i.e. c(w,d)\n",
    "doc_count, vocab_size = doc_term.shape\n",
    "vocab_table = vectorizer.vocabulary_  # Mapping of {word -> col of doc_term}\n",
    "\n",
    "# Process test set into doc-term matrix for fold-in usage\n",
    "doc_term_fi = vectorizer.transform(test_set).tocoo()\n",
    "doc_count_fi = doc_term_fi.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def plsa(doc_term_data, doc_term_row, doc_term_col, doc_topic, topic_word,\n",
    "         fold_in=False, max_iter=max_iter, threshold=threshold, verbose=1):\n",
    "    ''' PLSA implemented with sparse matrix and JIT compilation.\n",
    "    \n",
    "    Arguments are passed by references to functions defined with numba JIT,\n",
    "    thus you should manually initialize PLSA parameters and pass them in.\n",
    "    \n",
    "    Also, data, row, col attributes of COO matrix are directly passed in\n",
    "    since numba JIT does not recognize any type of scipy sparse matrix.\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        doc_term_data: Non-zero entry of doc-term matrix.\n",
    "        doc_term_row : Row index of entires in doc_term_data.\n",
    "        doc_term_col : Column index of entries in doc_term_data.\n",
    "        \n",
    "        doc_topic    : Numpy array in shape of (n_doc, n_topic).  i.e. P(T|d)\n",
    "        topic_word   : Numpy array in shape of (n_topic, n_word). i.e. P(w|T)\n",
    "        \n",
    "        fold_in      : Whether to perform fold-in strategy.\n",
    "        max_iter     : Max iterations of EM algorithm.\n",
    "        threshold    : Decide convergence of model for early stopping.\n",
    "            Training stops as (loglike - prev loglike) / |prev loglike| < threshold.\n",
    "            \n",
    "        verbose      : Control the frequency to print log-likelihood.\n",
    "                        <1: Print nothing.\n",
    "                         0: Print only at the end of training.\n",
    "                        >0: Print for every [verbose] iterations.\n",
    "    '''\n",
    "\n",
    "    ####################\n",
    "    #  Initialization\n",
    "    ####################\n",
    "    doc_count, topic_num = doc_topic.shape\n",
    "    vocab_size = topic_word.shape[1]\n",
    "    nnz = len(doc_term_data)  # Number of non-zero entries in doc-term matrix\n",
    "    \n",
    "    docword_topic = np.zeros((nnz, topic_num))  # P(T|w,d)  i.e E-step要算的東西\n",
    "    doc_topic_sum = np.zeros((doc_count))       # 更新P(T|d)時的分母項\n",
    "    if not fold_in:\n",
    "        topic_word_sum = np.zeros((topic_num))  # 更新P(w|T)時的分母項\n",
    "    \n",
    "    \n",
    "    prev_log_like = 0\n",
    "    for it in range(1, max_iter+1):\n",
    "        log_like = 0\n",
    "        ############\n",
    "        #  E-step\n",
    "        ############\n",
    "        for dwi in range(nnz):  # 更新P(T|w,d)時，對每一組非零之(w,d)逐個更新\n",
    "            di, wi = doc_term_row[dwi], doc_term_col[dwi]\n",
    "            joint_prob = np.zeros((topic_num))  # 分子項\n",
    "            joint_prob_sum = 0                  # 分母項\n",
    "            \n",
    "            for ti in range(topic_num): # P(T|w,d)的 T值也要逐個更新\n",
    "                joint_prob[ti] = doc_topic[di, ti] * topic_word[ti, wi] # P(w|T)P(T|d)\n",
    "                joint_prob_sum += joint_prob[ti]  # 把 P(w|T)P(T|d) 沿著 T 加總\n",
    "                \n",
    "            # log-likelihood (把目標函式擺在這計算比較有效率，不過也是要對每組(w,d)逐個加總)\n",
    "            log_like += doc_term_data[dwi] * np.log(joint_prob_sum)\n",
    "            \n",
    "            # Normalization  (i.e. 就是分子除以分母啦)\n",
    "            for ti in range(topic_num):\n",
    "                docword_topic[dwi, ti] = joint_prob[ti] / joint_prob_sum\n",
    "                \n",
    "                \n",
    "        # Early stopping\n",
    "        if prev_log_like != 0 and (log_like - prev_log_like) / abs(prev_log_like) < threshold:\n",
    "            if verbose > -1:\n",
    "                print(\"--- Early stopping at iteration\", it, \"---\")\n",
    "                print(\"loglike:\", log_like)\n",
    "                return\n",
    "            \n",
    "        if verbose > 0 and it % verbose == 0:\n",
    "            print(\"iter\", it, \"- loglike:\", log_like)\n",
    "        prev_log_like = log_like\n",
    "\n",
    "        \n",
    "        ##################################################\n",
    "        #  M-step  (如果你E-step看懂了，M-step一定也看得懂)\n",
    "        ##################################################\n",
    "        # 要更新的東西需要先全部歸零\n",
    "        doc_topic.fill(0)\n",
    "        doc_topic_sum.fill(0)\n",
    "        if not fold_in:\n",
    "            topic_word.fill(0)\n",
    "            topic_word_sum.fill(0)\n",
    "\n",
    "        for dwi in range(nnz):\n",
    "            di, wi = doc_term_row[dwi], doc_term_col[dwi]\n",
    "            for ti in range(topic_num):\n",
    "                likelihood = doc_term_data[dwi] * docword_topic[dwi, ti]  # c(w,d)P(T|w,d)\n",
    "                doc_topic[di, ti] += likelihood   # 因為index只用了di,ti，所以likelihood會沿著w加總\n",
    "                doc_topic_sum[di] += likelihood   # 同理，這次連ti都拔了，所以likelihood會沿著w與t加總\n",
    "                if not fold_in:\n",
    "                    topic_word[ti, wi] += likelihood  # (道理同上)\n",
    "                    topic_word_sum[ti] += likelihood\n",
    "                    \n",
    "        # Normalization\n",
    "        for di in range(doc_count):\n",
    "            for ti in range(topic_num):\n",
    "                doc_topic[di, ti] /= doc_topic_sum[di]\n",
    "        \n",
    "        if not fold_in:\n",
    "            for ti in range(topic_num):\n",
    "                for wi in range(vocab_size):\n",
    "                    topic_word[ti, wi] /= topic_word_sum[ti]  \n",
    "                \n",
    "    #################################################################\n",
    "    #  log-likelihood (如果沒有early stop，結束前會最後一次計算目標函式)\n",
    "    #################################################################\n",
    "    # 這邊的算法跟上面完全一樣\n",
    "    log_like = 0\n",
    "    for dwi in range(nnz):\n",
    "        di, wi = doc_term_row[dwi], doc_term_col[dwi]\n",
    "        joint_prob_sum = 0\n",
    "        for ti in range(topic_num):\n",
    "            joint_prob_sum += doc_topic[di, ti] * topic_word[ti, wi]\n",
    "        log_like += doc_term_data[dwi] * np.log(joint_prob_sum)\n",
    "        \n",
    "    if verbose > -1:\n",
    "        print(\"iter\", it, \"- loglike:\", log_like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(np_array, axis=-1):\n",
    "    ''' Sum of np_array along axis would be normalized to 1. '''\n",
    "    return np_array / np_array.sum(axis=-1, keepdims=True)\n",
    "\n",
    "# Randomize parameters of PLSA for both training and fold-in\n",
    "doc_topic_train = normalize(np.random.rand(doc_count, topic_num))   # P(T|d)\n",
    "topic_word = normalize(np.random.rand(topic_num, vocab_size))       # P(w|T)\n",
    "doc_topic_fi = normalize(np.random.rand(doc_count_fi, topic_num))   # P(T|d) for fold-in usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1 - loglike: -58375064.32942662\n",
      "iter 2 - loglike: -42666864.24795003\n",
      "iter 3 - loglike: -42631811.03370443\n",
      "iter 4 - loglike: -42589930.5870872\n",
      "iter 5 - loglike: -42533036.304297484\n",
      "iter 6 - loglike: -42453760.52602836\n",
      "iter 7 - loglike: -42342052.38765294\n",
      "iter 8 - loglike: -42179724.784964636\n",
      "iter 9 - loglike: -41944540.87725897\n",
      "iter 10 - loglike: -41624785.13707598\n",
      "iter 11 - loglike: -41246424.79358651\n",
      "iter 12 - loglike: -40866619.35683942\n",
      "iter 13 - loglike: -40530929.18316254\n",
      "iter 14 - loglike: -40253326.85668143\n",
      "iter 15 - loglike: -40027914.00614231\n",
      "iter 16 - loglike: -39843226.18056094\n",
      "iter 17 - loglike: -39689618.53012294\n",
      "iter 18 - loglike: -39560262.943060406\n",
      "iter 19 - loglike: -39450171.26666427\n",
      "iter 20 - loglike: -39355719.72334994\n",
      "iter 21 - loglike: -39274092.893165074\n",
      "iter 22 - loglike: -39203099.82021385\n",
      "iter 23 - loglike: -39141107.35095314\n",
      "iter 24 - loglike: -39086534.97153943\n",
      "iter 25 - loglike: -39038100.994601846\n",
      "iter 26 - loglike: -38994892.14180201\n",
      "iter 27 - loglike: -38956183.548511386\n",
      "iter 28 - loglike: -38921336.35531527\n",
      "iter 29 - loglike: -38889813.491955884\n",
      "iter 30 - loglike: -38861156.24205046\n",
      "iter 31 - loglike: -38835063.7222314\n",
      "iter 32 - loglike: -38811341.37886564\n",
      "iter 33 - loglike: -38789815.36786603\n",
      "iter 34 - loglike: -38770317.19486826\n",
      "iter 35 - loglike: -38752661.387732975\n",
      "iter 36 - loglike: -38736675.45090323\n",
      "iter 37 - loglike: -38722185.170385346\n",
      "iter 38 - loglike: -38708999.29382254\n",
      "iter 39 - loglike: -38696962.80754333\n",
      "iter 40 - loglike: -38685962.49920049\n",
      "iter 41 - loglike: -38675878.06694386\n",
      "iter 42 - loglike: -38666577.35603869\n",
      "iter 43 - loglike: -38657940.364039406\n",
      "iter 44 - loglike: -38649923.417630725\n",
      "iter 45 - loglike: -38642506.36219204\n",
      "iter 46 - loglike: -38635610.044443205\n",
      "iter 47 - loglike: -38629167.05800492\n",
      "iter 48 - loglike: -38623091.43595043\n",
      "iter 49 - loglike: -38617350.541056685\n",
      "iter 50 - loglike: -38611936.75323629\n",
      "iter 51 - loglike: -38606827.17683246\n",
      "iter 52 - loglike: -38601979.73565825\n",
      "iter 53 - loglike: -38597371.618690714\n",
      "iter 54 - loglike: -38593003.950613886\n",
      "iter 55 - loglike: -38588876.44135042\n",
      "iter 56 - loglike: -38584955.79931182\n",
      "--- Early stopping at iteration 57 ---\n",
      "loglike: -38581216.75605168\n",
      "CPU times: user 1min 53s, sys: 1.67 s, total: 1min 54s\n",
      "Wall time: 1min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "plsa(doc_term.data, doc_term.row, doc_term.col, doc_topic_train, topic_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1 - loglike: -3006588.0233709137\n",
      "iter 2 - loglike: -2882334.6224012002\n",
      "iter 3 - loglike: -2832016.2191949\n",
      "iter 4 - loglike: -2808913.0285848337\n",
      "iter 5 - loglike: -2797308.326754359\n",
      "iter 6 - loglike: -2790956.5827583554\n",
      "iter 7 - loglike: -2787201.909543563\n",
      "iter 8 - loglike: -2784834.618677411\n",
      "iter 9 - loglike: -2783262.733025315\n",
      "iter 10 - loglike: -2782175.165419598\n",
      "iter 11 - loglike: -2781397.0331342653\n",
      "iter 12 - loglike: -2780824.6267577736\n",
      "iter 13 - loglike: -2780393.9192611133\n",
      "iter 14 - loglike: -2780063.882542419\n",
      "--- Early stopping at iteration 15 ---\n",
      "loglike: -2779807.17344418\n",
      "CPU times: user 3.02 s, sys: 76 ms, total: 3.1 s\n",
      "Wall time: 3.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "plsa(doc_term_fi.data, doc_term_fi.row, doc_term_fi.col, doc_topic_fi, topic_word, fold_in=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.54 s, sys: 8.47 s, total: 16 s\n",
      "Wall time: 1.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "w_unigram = alpha * normalize(doc_term_fi.toarray())\n",
    "w_PLSA = beta * np.dot(doc_topic_fi, topic_word)\n",
    "\n",
    "w_BGLM ={}\n",
    "with open('BGLM.txt', mode='r') as file:\n",
    "    for line in file:\n",
    "        key, value = line.strip().split()\n",
    "        w_BGLM[key] = (1 - alpha - beta) * np.exp(np.float(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36 ms, sys: 4 ms, total: 40 ms\n",
      "Wall time: 39.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(\"submission.csv\", mode='w') as submit_file:\n",
    "    submit_file.write(\"Query,RetrievedDocuments\\n\")\n",
    "\n",
    "    query_list = os.listdir(query_dir)\n",
    "    for query_name in query_list:\n",
    "        submit_file.write(query_name + \",\")\n",
    "        \n",
    "        query_path = query_dir + query_name\n",
    "        with open(query_path) as query_file:\n",
    "            log_scores = np.zeros((doc_count_fi))\n",
    "            for line in query_file:\n",
    "                for word in line.rstrip().split()[:-1]:\n",
    "                    if word in vocab_table:\n",
    "                        wi = vocab_table[word]\n",
    "                        log_scores += np.log(w_unigram[:, wi] + w_PLSA[:, wi] + w_BGLM[word])\n",
    "                    else:\n",
    "                        log_scores += np.log(w_BGLM[word])\n",
    "                        \n",
    "        ranked_doc_idx = np.argsort(log_scores)[::-1]\n",
    "        for idx in ranked_doc_idx:\n",
    "            doc_name = doc_list[idx]\n",
    "            submit_file.write(\" \" + doc_name)\n",
    "        submit_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
